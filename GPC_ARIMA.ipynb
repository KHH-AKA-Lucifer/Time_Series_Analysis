{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libarries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection \n",
    "\n",
    "Download data first. Collect the data from yfinance API. The data should be 1-day period, 1-week period, 1-year period and 10-years period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one day - 1 min interval\n",
    "gold_one_day = yf.download('GC=F', interval=\"1m\")\n",
    "# one month - 5 min interval\n",
    "gold_one_month = yf.download('GC=F', interval=\"5m\", period=\"1mo\")\n",
    "# one year - 1 day interval\n",
    "gold_one_year = yf.download('GC=F', period=\"1y\")\n",
    "# ten years - 1 day interval \n",
    "gold_ten_year = yf.download('GC=F', period=\"10y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "Check the data to make sure no missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "for index, i in {\"gold_one_day\": gold_one_day, \"gold_one_month\": gold_one_month, \n",
    "                 \"gold_one_year\": gold_one_year, \"gold_one_year\": gold_one_year, \"gold_ten_year\": gold_ten_year}.items():\n",
    "    print(index)\n",
    "    print(\"------------\")\n",
    "    print(i.isna().sum(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! There is no missing value. Save the data set for later use. Need to update everyday for the latest data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data as csv file\n",
    "gold_one_day.to_csv('data/gold_one_day.csv')\n",
    "gold_one_month.to_csv('data/gold_one_month.csv')\n",
    "gold_one_year.to_csv('data/gold_one_year.csv')\n",
    "gold_ten_year.to_csv('data/gold_ten_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CSV files into dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data sets \n",
    "# load gold data for one day (1 min interval)\n",
    "df_one_day = pd.read_csv('data/gold_one_day.csv')\n",
    "# load gold data for one month (5 min interval)\n",
    "df_one_month = pd.read_csv('data/gold_one_month.csv')\n",
    "# load gold data for one year (1 day interval)\n",
    "df_one_year = pd.read_csv('data/gold_one_year.csv')\n",
    "# load gold data for one year (1 day interval)\n",
    "df_ten_year = pd.read_csv('data/gold_ten_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_year.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary columns named \"Adj Close\" and \"Volume\". Datatime column needs to be datatime data type. Convert it right away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns \n",
    "df_one_year.drop(columns=['Adj Close', 'Volume'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Datetime column to datetime datatype \n",
    "df_one_year['Date']= pd.to_datetime(df_one_year['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_year.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataframe and data type again \n",
    "print(df_one_year.head(3))\n",
    "\n",
    "print(df_one_year.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_one_year.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check the frequency \n",
    "df_one_year.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrangle(path, droped_columns):\n",
    "    \"\"\" A method that will clean the original dataset, \n",
    "        restructure the dataset and fill the missing values.\n",
    "        \n",
    "        input\n",
    "        -----\n",
    "        path: data path \n",
    "        dropped_columns: columns to be dropped\"\"\"\n",
    "    \n",
    "    # read the dataset through the path\n",
    "    df=pd.read_csv(path)\n",
    "    # change the \"Date\" column to datetime data type\n",
    "    df['Date']=pd.to_datetime(df['Date'])\n",
    "    # set the \"Date\" column to index\n",
    "    df=df.set_index('Date')\n",
    "    # assigned the desired frequecy to set up\n",
    "    # 'D' stands for day\n",
    "    desired_frequency = 'D'\n",
    "    # set the frequency \n",
    "    df = df.asfreq(desired_frequency)\n",
    "    # drop the unnecessary columns that are already specified \n",
    "    df = df.drop(columns=droped_columns)\n",
    "    # fill the missing values \n",
    "    df=df.fillna(method='ffill')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_year_1 = data_wrangle('data/gold_one_year.csv', ['Adj Close', 'Volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_year_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_one_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_one_year_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_year_1.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot the DataFrame\n",
    "df_one_year_1['Close'].plot(ax=ax)\n",
    "\n",
    "plt.title('Gold Prices Over Time (1-year 1-day interval)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "# Rotate x-axis labels 90 degrees\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot the DataFrame\n",
    "df_one_year_1['Open'].plot(ax=ax)\n",
    "\n",
    "plt.title('Gold Prices Over Time (1-year 1-day interval)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "# Rotate x-axis labels 90 degrees\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition \n",
    "\n",
    "Applying ARIMA directly do not give the desired output. Need to decompose the timeseries into trend, sesonality and residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_decomposition = seasonal_decompose(df_one_year_1['Close'], model='additive')\n",
    "# trend\n",
    "estimated_trend = ss_decomposition.trend\n",
    "# seasonal\n",
    "estimated_seasonal = ss_decomposition.seasonal\n",
    "# residual\n",
    "estimated_residual = ss_decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10))\n",
    "\n",
    "df_one_year_1['Close'].plot(ax=ax1, title='Original Time Series for Gold Price')\n",
    "ss_decomposition.trend.plot(ax=ax2, title='Trend')\n",
    "ss_decomposition.seasonal.plot(ax=ax3, title='Seasonal')\n",
    "ss_decomposition.resid.plot(ax=ax4, title='Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, sharex=True, sharey=False)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "axes[0].plot(df_one_year_1['Close'], label='Original')\n",
    "axes[0].legend(loc='upper left');\n",
    "\n",
    "axes[1].plot(estimated_trend, label='Trend')\n",
    "axes[1].legend(loc='upper left');\n",
    "\n",
    "axes[2].plot(estimated_seasonal, label='Seasonality')\n",
    "axes[2].legend(loc='upper left');\n",
    "\n",
    "axes[3].plot(estimated_residual, label='Residuals')\n",
    "axes[3].legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "#Histogram for the Residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(ss_decomposition.resid, bins=30, density=True, alpha=0.6, color='b')\n",
    "plt.title('Residuals Histogram')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Relative Frequency')\n",
    "\n",
    "#Q-Q plot of the residuals\n",
    "plt.subplot(1, 2, 2)\n",
    "sm.qqplot(ss_decomposition.resid, line='s', ax=plt.gca())\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plot_acf(ss_decomposition.resid)\n",
    "plt.title('ACF plot of residuals')\n",
    "plt.xlabel('Lags [Days]')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_decomposition.trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend=ss_decomposition.trend.dropna()\n",
    "trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plot_pacf(trend)\n",
    "plt.title('PACF plot of Trend Data')\n",
    "plt.xlabel('Lags [Days]')\n",
    "plt.ylabel('Autocorrelation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the trend data into the training set and the test set\n",
    "y_train=trend[:int(0.80*len(trend))]\n",
    "print(int(0.8*len(trend)))\n",
    "print(len(trend))\n",
    "y_test=trend[int(0.80*len(trend)):]\n",
    "\n",
    "\n",
    "#Here's the code for the forecast, using walk-forward validation\n",
    "\n",
    "y_prediction = pd.Series() #Starts an empty series to store the predicted values\n",
    "\n",
    "history = y_train.copy() #Training set starts with y_train, and gradually increases by 1 observation with each passing day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1+len(y_test)):\n",
    "\n",
    "    ARIMA_Model=ARIMA(history,order=(4,1,1)).fit() #Model is trained on history which increases with each loop\n",
    "\n",
    "    next_prediction=ARIMA_Model.forecast()  #Gives the prediction for the next timestamp\n",
    "    \n",
    "    y_prediction=pd.concat([y_prediction, next_prediction]) #Puts all the predictions and timestamps into the series y_prediction\n",
    "    \n",
    "    history=trend[:len(y_train)+i] #Training set increases by one observation in preparation for the next loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_days = 10\n",
    "# Creating a list to store future dates\n",
    "last_date = trend.index[-1]\n",
    "future_dates = pd.date_range(start=last_date, periods=future_days + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending the loop to include the future days\n",
    "for i in range(1, 1 + len(y_test) + future_days):\n",
    "    ARIMA_Model = ARIMA(history, order=(4, 1, 1)).fit()  # Model is trained on history which increases with each loop\n",
    "\n",
    "    next_prediction = ARIMA_Model.forecast()  # Gives the prediction for the next timestamp\n",
    "    \n",
    "    # Setting the index for the next prediction\n",
    "    if i <= len(y_test):\n",
    "        next_date = trend.index[len(y_train) + i - 1]\n",
    "    else:\n",
    "        next_date = future_dates[i - len(y_test) - 1]\n",
    "    \n",
    "    next_prediction.index = [next_date]\n",
    "    \n",
    "    # Puts all the predictions and timestamps into the series y_prediction\n",
    "    prediction = pd.concat([y_prediction, next_prediction])  \n",
    "    \n",
    "    if i <= len(y_test):\n",
    "        # Continue updating the history with actual test data\n",
    "        history = trend[:len(y_train) + i]  # Training set increases by one observation in preparation for the next loop\n",
    "    else:\n",
    "        # After test data, keep updating history with predictions\n",
    "        history = pd.concat([history, next_prediction])\n",
    "\n",
    "# Print the forecast for the future days\n",
    "print(prediction[-future_days:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(y_prediction)\n",
    "plt.plot(y_test)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(y_test, label=\"Test data\")\n",
    "plt.plot(prediction, label=\"Forecast\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_prediction)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_prediction))\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_year_1.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_year_1['prediction'] = y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_pacf(ss_decomposition.resid, lags=50)\n",
    "plt.title('ACF Plot of Residuals')\n",
    "plt.xlabel(\"Lags [Days]\", fontsize=15) \n",
    "plt.ylabel(\"Autocorrelation\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Stationarity \n",
    "\n",
    "It is obvious that the data is non-stationarity according to the visualization. But, to make sure check with the statistics method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with adfuller methods\n",
    "result = adfuller(df_one_year_1['Close'])\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing \n",
    "\n",
    "If p-value < 0.05, reject the null hypothesis.\n",
    "If not, fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ho: It is non stationary\n",
    "#H1: It is stationary\n",
    "\n",
    "def adfuller_test(df):\n",
    "    \"\"\"A method for testing hypothesis for data stationarity.\n",
    "\n",
    "        input\n",
    "        -----\n",
    "        df: dataframe \n",
    "\n",
    "        output\n",
    "        ------\n",
    "        ADF: \n",
    "        p-value: the significant value\n",
    "        Lags: the significant lags / spikes\n",
    "        No.of observation: the numbers of lags that observe\n",
    "    \"\"\"\n",
    "    # assign the column into Augmented Dickey-Fuller Test (ADF)\n",
    "    result=adfuller(df)\n",
    "    # creat a list of labels \n",
    "    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n",
    "    # zip the value and label together \n",
    "    for value,label in zip(result,labels):\n",
    "        # print the label and value \n",
    "        print(label+' : '+str(value) )\n",
    "    # if p-value is less than 0.05,\n",
    "    if result[1] <= 0.05:\n",
    "        # reject the null hypothesis \n",
    "        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n",
    "    # if not,\n",
    "    else:\n",
    "        # fail to reject null hypothesis\n",
    "        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller_test(df_one_year_1['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differencing\n",
    "\n",
    "Since the original is non-stationarity, we need to difference the time series data to determine degree of integration \"d\" to make the data stationarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first order differencing \n",
    "df_one_year_1d = df_one_year_1['Close'].diff()\n",
    "# test the hypothesis\n",
    "adfuller_test(df_one_year_1d.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data becomes sationary. To make sure, find the second order differencing of the data too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first order differencing \n",
    "df_one_year_2d = df_one_year_1['Close'].diff().diff()\n",
    "# test the hypothesis\n",
    "adfuller_test(df_one_year_2d.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data become stationary at degree of integration one, we can choose one as our \"d\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining \"d\", we need to consider \"p\" and \"q\". We need to visualize ACFs and PACFs to determine those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acf \n",
    "num_lags = 20\n",
    "acf_values = acf(df_one_year_1.Close.diff().dropna())\n",
    "plt.bar(range(num_lags), acf_values[:num_lags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pacf \n",
    "num_lags = 20\n",
    "pacf_values = pacf(df_one_year_1.Close.diff().dropna())\n",
    "plt.bar(range(num_lags), pacf_values[:num_lags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first order differencing\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax1 = fig.add_subplot(131)  # Corrected typo here\n",
    "ax1.set_title('1st Order Differencing')\n",
    "ax1.plot(df_one_year_1.Close.diff());\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "plot_acf(df_one_year_1.Close.diff(), ax=ax2, lags=20);\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "plot_pacf(df_one_year_1.Close.diff(), ax=ax3, lags=20);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the ACFs and PACFs, there is one lag at PACFs. Therefore, p = 1. No signigicant lag in ACFs. The value of \"q\" is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_one_year_1.Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test data \n",
    "train_data = int(len(df_one_year_1.Close) * 0.8)\n",
    "test_data = int(len(df_one_year_1.Close)-train_data)\n",
    "print(\"No. of train data: \",train_data)\n",
    "print(\"No. of test data:\", test_data)\n",
    "train_data_df = df_one_year_1[0:train_data]\n",
    "test_data_df = df_one_year_1[train_data:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "stepwise_model = auto_arima(train_data_df['Close'], start_p=1, start_q=1,\n",
    "                            max_p=3, max_q=3, seasonal=False,\n",
    "                            d=1, trace=True, error_action='ignore',\n",
    "                            suppress_warnings=True, stepwise=True)\n",
    "\n",
    "print(stepwise_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to autoarima, the best parameters are p = 2, d = 1 and q = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 2, d = 1, q = 2\n",
    "\n",
    "model = ARIMA(train_data_df.Close, order=(2, 1, 2))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample predictions\n",
    "in_sample_pred = model_fit.predict(start=train_data_df.index[0], end=train_data_df.index[-1])\n",
    "\n",
    "# Plot in-sample predictions vs actual values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data_df.index, train_data_df['Close'], label='Actual')\n",
    "plt.plot(train_data_df.index, in_sample_pred, label='In-Sample Prediction', color='red')\n",
    "plt.title('In-Sample Prediction vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the predictions and residuals\n",
    "model_fit.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual plotting\n",
    "residuals = model_fit.resid\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(residuals)\n",
    "plt.title('Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_acf(residuals)\n",
    "plot_pacf(residuals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = model_fit.resid\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "plot_acf(residuals, lags=20, ax=ax[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decomposition = seasonal_decompose(train_data_df['Close'], model='additive')\n",
    "# trend\n",
    "train_trend = train_decomposition.trend\n",
    "# seasonal\n",
    "train_seasonal = train_decomposition.seasonal\n",
    "# residual\n",
    "train_residual = train_decomposition.resid\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, sharex=True, sharey=False)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "axes[0].plot(train_data_df['Close'], label='Original')\n",
    "axes[0].legend(loc='upper left');\n",
    "\n",
    "axes[1].plot(train_trend, label='Trend')\n",
    "axes[1].legend(loc='upper left');\n",
    "\n",
    "axes[2].plot(train_seasonal, label='Seasonality')\n",
    "axes[2].legend(loc='upper left');\n",
    "\n",
    "axes[3].plot(train_residual, label='Residuals')\n",
    "axes[3].legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decomposition = seasonal_decompose(test_data_df['Close'], model='additive')\n",
    "# trend\n",
    "test_trend = test_decomposition.trend\n",
    "# seasonal\n",
    "test_seasonal = test_decomposition.seasonal\n",
    "# residual\n",
    "test_residual = test_decomposition.resid\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, sharex=True, sharey=False)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "axes[0].plot(test_data_df['Close'], label='Original')\n",
    "axes[0].legend(loc='upper left');\n",
    "\n",
    "axes[1].plot(test_trend, label='Trend')\n",
    "axes[1].legend(loc='upper left');\n",
    "\n",
    "axes[2].plot(test_seasonal, label='Seasonality')\n",
    "axes[2].legend(loc='upper left');\n",
    "\n",
    "axes[3].plot(test_residual, label='Residuals')\n",
    "axes[3].legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-sample forecast\n",
    "forecast_steps = len(test_data_df)\n",
    "out_sample_forecast = model_fit.forecast(steps=forecast_steps)\n",
    "\n",
    "# Plot forecast vs actual values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data_df.index, train_data_df['Close'], label='Train')\n",
    "plt.plot(test_data_df.index, test_data_df['Close'], label='Test')\n",
    "plt.plot(test_data_df.index, out_sample_forecast, label='Forecast', color='red')\n",
    "plt.title('Out-of-Sample Forecast vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecast vs actual values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data_df.index, train_data_df['Close'], label='Train')\n",
    "plt.plot(test_data_df.index, test_data_df['Close'], label='Test')\n",
    "plt.plot(df_one_year_1.index, df_one_year_1.prediction, label='Forecast', color='red')\n",
    "plt.title('Out-of-Sample Forecast vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print evaluation metrics\n",
    "mae = mean_absolute_error(test_data_df['Close'], out_sample_forecast)\n",
    "rmse = np.sqrt(mean_squared_error(test_data_df['Close'], out_sample_forecast))\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df[\"Prediction\"] = model_fit.forecast(len(test_data_df))\n",
    "\n",
    "test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = test_data_df['Close'] - out_sample_forecast\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(residuals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 8))\n",
    "model.fit().plot_diagnostics()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_fit.forecast()\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
